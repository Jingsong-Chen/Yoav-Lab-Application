{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avKQb-BLP-l6"
   },
   "source": [
    "# Link to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d40OpOs1H9Dd",
    "outputId": "9aa8d137-2e31-4469-b68d-46c3328448c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/My Drive/Colab Notebooks\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd '/content/drive/My Drive/Colab Notebooks'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qQXoZl9KKbW3"
   },
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "44wE6q7TUnx3"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ru12oE5rZLqF"
   },
   "source": [
    "### Translation blocks are sparated by empty lines:\n",
    "\n",
    "Block 1\\n\n",
    "\n",
    "\\n\n",
    "\n",
    "Block 2\\n\n",
    "\n",
    "\\n\n",
    "\n",
    "...\n",
    "\n",
    "Block N\\n\n",
    "\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1RTbXKZQLSK9",
    "outputId": "4b07f80b-01ac-49e2-d562-fccf2d285032"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "发言人 表示 , 美国国会 众议院 国际 关系 委员会 主席 海德 先生 2 日 在 港 发表 的 演讲 , 对 中国 的 发展 与 进步 进行 了 恶意 攻击 , 并 以 冷战 思维 污蔑 中国 外交 政策 , 我们 表示 强烈 不满 .\n",
      "the spokesman said the speech in hong kong on the 2 nd by mr. hyde , chairman of the us house of representatives ' international relations committee , was a malicious attack on china's development and progress as well as a slander on china's foreign policy evidencing a cold war mentality , with which we are strongly unsatisfied .\n",
      "the spokesman claimed that china had expressed its strong displeasure with the speech delivered by mr. hyde , chairman of the u.s. house international relations committee , in hong kong on december 2 , which contained malicious attacks on china's development and progress , and tarnished china's foreign policy with his cold war mentality .\n",
      "0.6332\n",
      "H\n",
      "There are 584 training translation pairs.\n",
      "\n",
      "域 名 是 开展 电子 商务 , 电子 政务 等 一切 互 联 网 应用 的 通信 基础 , 目前 被 广泛 使用 作为 互 联 网 地址 .\n",
      "domain names are the basis of communications on which all internet sites use for e-business , e-government , and other applications . at present , they are widely used as internet addresses .\n",
      "a domain name is the communication basis for all internet applications including the conducting of electronic business and electronic government affairs . at present , it is widely used for internet addresses .\n",
      "0.5455\n",
      "H\n",
      "There are 174 testing translation pairs.\n"
     ]
    }
   ],
   "source": [
    "# read the text file - \n",
    "train_file_path = \"train.txt\"\n",
    "test_file_path = \"test.txt\"\n",
    "blocks_train = open(train_file_path, \"r\").read().split(\"\\n\\n\")\n",
    "blocks_test = open(test_file_path, \"r\").read().split(\"\\n\\n\")\n",
    "\n",
    "# remove the trailing empty line at the end of each file\n",
    "blocks_train[-1] = blocks_train[-1][:-1]\n",
    "blocks_test[-1] = blocks_test[-1][:-1]\n",
    "\n",
    "# check the last translation pair to make sure that there is no misreading\n",
    "print(blocks_train[-1])\n",
    "print('There are', len(blocks_train), 'training translation pairs.\\n')\n",
    "print(blocks_test[-1])\n",
    "print('There are', len(blocks_test), 'testing translation pairs.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwTLcYhnYZ43"
   },
   "source": [
    "### Each translation block has 5 lines:\n",
    "*   line 1: source (Chinese)\n",
    "*   line 2: translation1 (English)\n",
    "*   line 3: translation2 (English)\n",
    "*   line 4: probablility? (float)\n",
    "*   line 5: label (human/machine)\n",
    "\n",
    "with one exception: the second English translation of the 83th block:\n",
    "\n",
    "`although the arms embargo to the chinese 丨 shadow on the eu summit , but still in the two sides signed several trade and other agreements .`\n",
    "\n",
    "has one Chinese character '丨' (U+4E28). I had to manually exclude it.\n",
    "\n",
    "Also, I encode label 'H' as 0 and 'M' as 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMtkNvTpSh0f"
   },
   "outputs": [],
   "source": [
    "# detects if texts have Chinese characters\n",
    "def isChinese(texts):\n",
    "  # exclude the '丨' character\n",
    "  if re.search(\"[\\u4e00-\\u4e27\\u4e29-\\u9FFF]\", texts):\n",
    "      return True\n",
    "  return False\n",
    "\n",
    "# parse translation block strings\n",
    "def parseBlocks(blocks):\n",
    "  chinese = []\n",
    "  reference = []\n",
    "  english_translation = []\n",
    "  probablilities = []\n",
    "  labels_translation = []\n",
    "\n",
    "  i = 0\n",
    "  while i < len(blocks):\n",
    "    lines = blocks[i].split('\\n')\n",
    "    # the first setence is Chinese\n",
    "    chinese.append(lines[0])\n",
    "    # the first Englisth translation is done human\n",
    "    reference.append(lines[1])\n",
    "    # the second English translation can be machine translation\n",
    "    english_translation.append(lines[2])\n",
    "    lines[4] = 0 if lines[4] == 'H' else 1\n",
    "    labels_translation.append(lines[4])\n",
    "    # append the quality score\n",
    "    probablilities.append(float(lines[3])) \n",
    "    i += 1\n",
    "  return chinese, english_translation, labels_translation, reference, probablilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ov7DYvZPIPZS",
    "outputId": "e41f37eb-76c0-4614-95c3-b4b3dfefcce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** Length check for the training info:\n",
      "584\n",
      "584\n",
      "584\n",
      "584\n",
      "584 \n",
      "\n",
      "******** Content check for the training info:\n",
      "奈 斯 说 , 计 画 怀孕 的 女性 仍 应 服用 叶酸 补品 .\n",
      "ness says women planning to become pregnant should still take folic acid supplements .\n",
      "ness said that women planning to become pregnant should continue to take folic acid supplements .\n",
      "0.7500\n",
      "H\n",
      "奈 斯 说 , 计 画 怀孕 的 女性 仍 应 服用 叶酸 补品 .\n",
      "ness said that women planning to become pregnant should continue to take folic acid supplements .\n",
      "0\n",
      "ness says women planning to become pregnant should still take folic acid supplements .\n",
      "0.75\n"
     ]
    }
   ],
   "source": [
    "chinese_train,\\\n",
    "english_translation_train,\\\n",
    "labels_translation_train,\\\n",
    "reference_train,\\\n",
    "probablilities_train = parseBlocks(blocks_train)\n",
    "labels_translation_train = np.array(labels_translation_train)\n",
    "labels_reference_train = np.array([0] * len(reference_train))\n",
    "\n",
    "# sanity check\n",
    "print('******** Length check for the training info:')\n",
    "print(len(chinese_train))\n",
    "print(len(english_translation_train))\n",
    "print(len(labels_translation_train))\n",
    "print(len(reference_train))\n",
    "print(len(probablilities_train), '\\n')\n",
    "\n",
    "\n",
    "print('******** Content check for the training info:')\n",
    "print(blocks_train[99])\n",
    "print(chinese_train[99])\n",
    "print(english_translation_train[99])\n",
    "print(labels_translation_train[99])\n",
    "print(reference_train[99])\n",
    "print(probablilities_train[99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EU9OfaOZP1Yr",
    "outputId": "c5bb9cda-56b9-459a-e3f3-396f90c1313b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** Length check for the training info:\n",
      "174\n",
      "174\n",
      "174\n",
      "174\n",
      "174 \n",
      "\n",
      "******** Content check for the training info:\n",
      "国际 计划 1995 年 进入 中国 , 迄今 已 资助 儿童 21300 多 名 .\n",
      "plan international entered china in 1995 and has helped over 213000 children so far .\n",
      "plan international entered china in 1995 . to date , it has subsidized over 21300 children .\n",
      "0.5882\n",
      "H\n",
      "国际 计划 1995 年 进入 中国 , 迄今 已 资助 儿童 21300 多 名 .\n",
      "plan international entered china in 1995 . to date , it has subsidized over 21300 children .\n",
      "0\n",
      "plan international entered china in 1995 and has helped over 213000 children so far .\n",
      "0.5882\n"
     ]
    }
   ],
   "source": [
    "chinese_test,\\\n",
    "english_translation_test,\\\n",
    "labels_translation_test,\\\n",
    "reference_test,\\\n",
    "probablilities_test = parseBlocks(blocks_test)\n",
    "labels_translation_test = np.array(labels_translation_test)\n",
    "labels_reference_test = np.array([0] * len(reference_test))\n",
    "\n",
    "# sanity check\n",
    "print('******** Length check for the training info:')\n",
    "print(len(chinese_test))\n",
    "print(len(english_translation_test))\n",
    "print(len(labels_translation_test))\n",
    "print(len(reference_test))\n",
    "print(len(probablilities_test), '\\n')\n",
    "\n",
    "\n",
    "print('******** Content check for the training info:')\n",
    "print(blocks_test[99])\n",
    "print(chinese_test[99])\n",
    "print(english_translation_test[99])\n",
    "print(labels_translation_test[99])\n",
    "print(reference_test[99])\n",
    "print(probablilities_test[99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JlN0T992j3D"
   },
   "source": [
    "## Word embedding\n",
    "\n",
    "### I use SentenceBERT to transform sentences into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73JFM0xNySgW",
    "outputId": "e289c9b3-a769-4a45-d53a-b182580734b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/e2/84d6acfcee2d83164149778a33b6bdd1a74e1bcb59b2b2cd1b861359b339/sentence-transformers-0.4.1.2.tar.gz (64kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 3.2MB/s \n",
      "\u001b[?25hCollecting transformers<5.0.0,>=3.1.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8MB 5.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n",
      "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.7.0+cu101)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.19.5)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/67/e42bd1181472c95c8cda79305df848264f2a7f62740995a46945d9797b67/sentencepiece-0.1.95-cp36-cp36m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 21.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2019.12.20)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 27.6MB/s \n",
      "\u001b[?25hCollecting tokenizers==0.9.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9MB 32.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (20.9)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (2.23.0)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers<5.0.0,>=3.1.0->sentence-transformers) (0.8)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (1.0.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<5.0.0,>=3.1.0->sentence-transformers) (7.1.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<5.0.0,>=3.1.0->sentence-transformers) (3.0.4)\n",
      "Building wheels for collected packages: sentence-transformers, sacremoses\n",
      "  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sentence-transformers: filename=sentence_transformers-0.4.1.2-cp36-none-any.whl size=103068 sha256=498232f438ff9283ebc752dc209b26f6d067557b34c92e971149d42bd4ba2d87\n",
      "  Stored in directory: /root/.cache/pip/wheels/3d/33/d1/5703dd56199c09d4a1b41e0c07fb4e7765a84d787cbdc48ac3\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=957a658888f30aad93da7b3befa03f560ff6dcc485cc977108878f5636404e9b\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sentence-transformers sacremoses\n",
      "Installing collected packages: sacremoses, tokenizers, transformers, sentencepiece, sentence-transformers\n",
      "Successfully installed sacremoses-0.0.43 sentence-transformers-0.4.1.2 sentencepiece-0.1.95 tokenizers-0.9.4 transformers-4.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25oBdY3n29TD"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c-MCCznt6vxm",
    "outputId": "f6bbc1b9-6505-412a-e5c7-3bc1f838e70f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 405M/405M [00:46<00:00, 8.75MB/s]\n"
     ]
    }
   ],
   "source": [
    "# choose model\n",
    "sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s3j4Lzhc4S_E",
    "outputId": "061d42f1-e0f7-43d3-c3e5-db817d611b00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(584, 768)\n",
      "(174, 768)\n"
     ]
    }
   ],
   "source": [
    "# # vectorize sentences\n",
    "# english_translation_embeddings = sbert_model.encode(english_translation_train + english_translation_test)\n",
    "# english_translation_embeddings_train = english_translation_embeddings[:len(english_translation_train)] # \n",
    "# english_translation_embeddings_test = english_translation_embeddings[len(english_translation_train):] # \n",
    "\n",
    "# # save the embeddings\n",
    "path_translation_embeddings_train = 'translation_embeddings_train_bert.p'\n",
    "path_translation_embeddings_test = 'translation_embeddings_test_bert.p'\n",
    "# pickle.dump(english_translation_embeddings_train, open(path_translation_embeddings_train, \"wb\"))\n",
    "# pickle.dump(english_translation_embeddings_test, open(path_translation_embeddings_test, \"wb\"))\n",
    "\n",
    "# load the pickled embeddings\n",
    "english_translation_embeddings_train = pickle.load(open(path_translation_embeddings_train, \"rb\"))\n",
    "english_translation_embeddings_test = pickle.load(open(path_translation_embeddings_test, \"rb\"))\n",
    "\n",
    "# sanity check\n",
    "print(english_translation_embeddings_train.shape)\n",
    "print(english_translation_embeddings_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFiqZtUjP4Ok"
   },
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66xdbRbzt_Uu"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import keras\n",
    "import sklearn\n",
    "from keras.layers import Input, Dense, Flatten, Dropout, Reshape, LSTM, Bidirectional\n",
    "from keras.layers import Embedding, Concatenate\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU,ReLU\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "import keras.backend as K\n",
    "from keras.models import Model, Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-L_oN63GxDNl"
   },
   "source": [
    "Use a neuro-network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YK2PZ63eKqbL",
    "outputId": "c91e19cd-2d54-493e-db5b-3c688375b2a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sentence_input (InputLayer)  [(None, 768, 1)]          0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 768, 128)          256       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 768, 128)          0         \n",
      "_________________________________________________________________\n",
      "re_lu (ReLU)                 (None, 768, 128)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 768, 256)          263168    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 256)               394240    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 690,689\n",
      "Trainable params: 690,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sentence_input = Input(shape=(768,1), name='sentence_input')\n",
    "\n",
    "droprate = 0.3\n",
    "dimension = 128\n",
    "\n",
    "x = Dense(dimension)(sentence_input)\n",
    "x = Dropout(droprate)(x)\n",
    "# x = LeakyReLU(alpha=0.01)(x)\n",
    "x = ReLU()(x)\n",
    "\n",
    "x = Bidirectional(LSTM(dimension, return_sequences=True, activation='tanh'))(x) \n",
    "x = Bidirectional(LSTM(dimension, return_sequences=False, activation='tanh'))(x)\n",
    "\n",
    "x = Dropout(droprate)(x)\n",
    "x = Dense(dimension)(x)\n",
    "x = ReLU()(x)\n",
    "\n",
    "output = Dense(1, activation='softmax')(x)\n",
    "discriminator = Model(sentence_input ,output)\n",
    "discriminator.summary()\n",
    "\n",
    "optimizer = Adam(lr=0.001)\n",
    "discriminator.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EM6JzBiLxZ3S"
   },
   "source": [
    "Train a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fghXoFnM8CqO"
   },
   "outputs": [],
   "source": [
    "def train(batch_size, iterations, training_data, training_label, testing_data, testing_label, model):\n",
    "  batch_number = len(training_data) // batch_size\n",
    "  avg_f1_score = 0\n",
    "  for epoch in range(iterations):\n",
    "    for i in range(batch_number):\n",
    "      indices = np.random.choice(training_data.shape[0], batch_size, replace=False)\n",
    "      batch_data = training_data[indices]\n",
    "      batch_label = training_label[indices]\n",
    "      # print(data.shape)\n",
    "      # print(label_batch.shape)\n",
    "      loss = model.train_on_batch(batch_data,batch_label)\n",
    "      if i % 5 == 0:\n",
    "        print('epoch', epoch, 'batch', i)\n",
    "        print('loss & accuracy', loss)\n",
    "    # print out progress every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "      print('+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')\n",
    "      print('epoch', epoch)\n",
    "      indices = np.random.choice(training_data.shape[0], batch_size, replace=False)\n",
    "      batch_data = training_data[indices]\n",
    "      batch_label = training_label[indices]\n",
    "      predoctions = model.predict_on_batch(batch_data)\n",
    "      # calculate F1 score\n",
    "      f1 = sklearn.metrics.f1_score(batch_label, predoctions)\n",
    "      print('f1 score', f1)\n",
    "      avg_f1_score = (avg_f1_score * epoch / 5 + f1) / (epoch / 5 + 1)\n",
    "      print('average f1 score', avg_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "_mReM-3yhkzA",
    "outputId": "c5d6f08b-3142-4e28-9e53-0dc1440e87aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 batch 0\n",
      "loss & accuracy [0.6927883625030518, 0.53125]\n",
      "epoch 0 batch 5\n",
      "loss & accuracy [0.6972246766090393, 0.5]\n",
      "epoch 0 batch 10\n",
      "loss & accuracy [0.689956784248352, 0.4375]\n",
      "epoch 0 batch 15\n",
      "loss & accuracy [0.6913466453552246, 0.46875]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "epoch 0\n",
      "f1 score 0.6666666666666666\n",
      "average f1 score 0.6666666666666666\n",
      "epoch 1 batch 0\n",
      "loss & accuracy [0.6990156769752502, 0.40625]\n",
      "epoch 1 batch 5\n",
      "loss & accuracy [0.6980094313621521, 0.5]\n",
      "epoch 1 batch 10\n",
      "loss & accuracy [0.6923913955688477, 0.46875]\n",
      "epoch 1 batch 15\n",
      "loss & accuracy [0.68578040599823, 0.34375]\n",
      "epoch 2 batch 0\n",
      "loss & accuracy [0.7101274728775024, 0.5625]\n",
      "epoch 2 batch 5\n",
      "loss & accuracy [0.6889208555221558, 0.46875]\n",
      "epoch 2 batch 10\n",
      "loss & accuracy [0.6946236491203308, 0.5]\n",
      "epoch 2 batch 15\n",
      "loss & accuracy [0.7349269390106201, 0.5]\n",
      "epoch 3 batch 0\n",
      "loss & accuracy [0.7498829364776611, 0.53125]\n",
      "epoch 3 batch 5\n",
      "loss & accuracy [0.6870465278625488, 0.46875]\n",
      "epoch 3 batch 10\n",
      "loss & accuracy [0.6952415704727173, 0.5]\n",
      "epoch 3 batch 15\n",
      "loss & accuracy [0.6898808479309082, 0.4375]\n",
      "epoch 4 batch 0\n",
      "loss & accuracy [0.6893054246902466, 0.46875]\n",
      "epoch 4 batch 5\n",
      "loss & accuracy [0.6907851696014404, 0.4375]\n",
      "epoch 4 batch 10\n",
      "loss & accuracy [0.6740640997886658, 0.375]\n",
      "epoch 4 batch 15\n",
      "loss & accuracy [0.6169044375419617, 0.3125]\n",
      "epoch 5 batch 0\n",
      "loss & accuracy [0.669216513633728, 0.40625]\n",
      "epoch 5 batch 5\n",
      "loss & accuracy [0.6982278823852539, 0.53125]\n",
      "epoch 5 batch 10\n",
      "loss & accuracy [0.6961315870285034, 0.65625]\n",
      "epoch 5 batch 15\n",
      "loss & accuracy [0.6735436320304871, 0.40625]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "epoch 5\n",
      "f1 score 0.6086956521739131\n",
      "average f1 score 0.6376811594202898\n",
      "epoch 6 batch 0\n",
      "loss & accuracy [0.6945679187774658, 0.5]\n",
      "epoch 6 batch 5\n",
      "loss & accuracy [0.689537763595581, 0.53125]\n",
      "epoch 6 batch 10\n",
      "loss & accuracy [0.7140794992446899, 0.5625]\n",
      "epoch 6 batch 15\n",
      "loss & accuracy [0.6817702054977417, 0.53125]\n",
      "epoch 7 batch 0\n",
      "loss & accuracy [0.6284381151199341, 0.34375]\n",
      "epoch 7 batch 5\n",
      "loss & accuracy [0.7200430631637573, 0.53125]\n",
      "epoch 7 batch 10\n",
      "loss & accuracy [0.6964811682701111, 0.40625]\n",
      "epoch 7 batch 15\n",
      "loss & accuracy [0.6646499633789062, 0.21875]\n",
      "epoch 8 batch 0\n",
      "loss & accuracy [0.7031682729721069, 0.4375]\n",
      "epoch 8 batch 5\n",
      "loss & accuracy [0.6680634021759033, 0.375]\n",
      "epoch 8 batch 10\n",
      "loss & accuracy [0.6912130117416382, 0.5]\n",
      "epoch 8 batch 15\n",
      "loss & accuracy [0.6694564819335938, 0.375]\n",
      "epoch 9 batch 0\n",
      "loss & accuracy [0.6999446153640747, 0.53125]\n",
      "epoch 9 batch 5\n",
      "loss & accuracy [0.6833011507987976, 0.5]\n",
      "epoch 9 batch 10\n",
      "loss & accuracy [0.6945406794548035, 0.5]\n",
      "epoch 9 batch 15\n",
      "loss & accuracy [0.6499112844467163, 0.4375]\n",
      "epoch 10 batch 0\n",
      "loss & accuracy [0.7056705951690674, 0.46875]\n",
      "epoch 10 batch 5\n",
      "loss & accuracy [0.7084060907363892, 0.40625]\n",
      "epoch 10 batch 10\n",
      "loss & accuracy [0.6833317279815674, 0.46875]\n",
      "epoch 10 batch 15\n",
      "loss & accuracy [0.6692100763320923, 0.4375]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "epoch 10\n",
      "f1 score 0.5454545454545454\n",
      "average f1 score 0.6069389547650417\n",
      "epoch 11 batch 0\n",
      "loss & accuracy [0.668097972869873, 0.5]\n",
      "epoch 11 batch 5\n",
      "loss & accuracy [0.6730538606643677, 0.375]\n",
      "epoch 11 batch 10\n",
      "loss & accuracy [0.715572714805603, 0.53125]\n",
      "epoch 11 batch 15\n",
      "loss & accuracy [0.6982035040855408, 0.5625]\n",
      "epoch 12 batch 0\n",
      "loss & accuracy [0.7103734016418457, 0.71875]\n",
      "epoch 12 batch 5\n",
      "loss & accuracy [0.6545394659042358, 0.625]\n",
      "epoch 12 batch 10\n",
      "loss & accuracy [0.7040178775787354, 0.46875]\n",
      "epoch 12 batch 15\n",
      "loss & accuracy [0.6521835327148438, 0.59375]\n",
      "epoch 13 batch 0\n",
      "loss & accuracy [0.6972742080688477, 0.46875]\n",
      "epoch 13 batch 5\n",
      "loss & accuracy [0.6606408357620239, 0.375]\n",
      "epoch 13 batch 10\n",
      "loss & accuracy [0.7029361724853516, 0.5625]\n",
      "epoch 13 batch 15\n",
      "loss & accuracy [0.6982872486114502, 0.5]\n",
      "epoch 14 batch 0\n",
      "loss & accuracy [0.6951682567596436, 0.59375]\n",
      "epoch 14 batch 5\n",
      "loss & accuracy [0.6804189682006836, 0.40625]\n",
      "epoch 14 batch 10\n",
      "loss & accuracy [0.61300128698349, 0.34375]\n",
      "epoch 14 batch 15\n",
      "loss & accuracy [0.6866201758384705, 0.40625]\n",
      "epoch 15 batch 0\n",
      "loss & accuracy [0.7267276048660278, 0.5625]\n",
      "epoch 15 batch 5\n",
      "loss & accuracy [0.689987301826477, 0.40625]\n",
      "epoch 15 batch 10\n",
      "loss & accuracy [0.6473897695541382, 0.5625]\n",
      "epoch 15 batch 15\n",
      "loss & accuracy [0.6538306474685669, 0.4375]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "epoch 15\n",
      "f1 score 0.72\n",
      "average f1 score 0.6352042160737812\n",
      "epoch 16 batch 0\n",
      "loss & accuracy [0.5832605361938477, 0.40625]\n",
      "epoch 16 batch 5\n",
      "loss & accuracy [0.5600104331970215, 0.34375]\n",
      "epoch 16 batch 10\n",
      "loss & accuracy [0.733457088470459, 0.5]\n",
      "epoch 16 batch 15\n",
      "loss & accuracy [0.6133931279182434, 0.4375]\n",
      "epoch 17 batch 0\n",
      "loss & accuracy [0.6702989339828491, 0.53125]\n",
      "epoch 17 batch 5\n",
      "loss & accuracy [0.6333378553390503, 0.53125]\n",
      "epoch 17 batch 10\n",
      "loss & accuracy [0.5863056778907776, 0.53125]\n",
      "epoch 17 batch 15\n",
      "loss & accuracy [0.7363733053207397, 0.375]\n",
      "epoch 18 batch 0\n",
      "loss & accuracy [0.6543097496032715, 0.46875]\n",
      "epoch 18 batch 5\n",
      "loss & accuracy [0.6500483751296997, 0.40625]\n",
      "epoch 18 batch 10\n",
      "loss & accuracy [0.6737927198410034, 0.5625]\n",
      "epoch 18 batch 15\n",
      "loss & accuracy [0.6685096621513367, 0.375]\n",
      "epoch 19 batch 0\n",
      "loss & accuracy [0.6639943718910217, 0.53125]\n",
      "epoch 19 batch 5\n",
      "loss & accuracy [0.6659031510353088, 0.5]\n",
      "epoch 19 batch 10\n",
      "loss & accuracy [0.6148512959480286, 0.5625]\n",
      "epoch 19 batch 15\n",
      "loss & accuracy [0.5947834849357605, 0.59375]\n",
      "epoch 20 batch 0\n",
      "loss & accuracy [0.7400726675987244, 0.5625]\n",
      "epoch 20 batch 5\n",
      "loss & accuracy [0.6689050197601318, 0.375]\n",
      "epoch 20 batch 10\n",
      "loss & accuracy [0.6229385733604431, 0.375]\n",
      "epoch 20 batch 15\n",
      "loss & accuracy [0.7292690873146057, 0.5625]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "epoch 20\n",
      "f1 score 0.4\n",
      "average f1 score 0.588163372859025\n",
      "epoch 21 batch 0\n",
      "loss & accuracy [0.638644814491272, 0.46875]\n",
      "epoch 21 batch 5\n",
      "loss & accuracy [0.6078008413314819, 0.34375]\n",
      "epoch 21 batch 10\n",
      "loss & accuracy [0.6255771517753601, 0.4375]\n",
      "epoch 21 batch 15\n",
      "loss & accuracy [0.6382327079772949, 0.46875]\n",
      "epoch 22 batch 0\n",
      "loss & accuracy [0.6834996342658997, 0.46875]\n",
      "epoch 22 batch 5\n",
      "loss & accuracy [0.630850076675415, 0.4375]\n",
      "epoch 22 batch 10\n",
      "loss & accuracy [0.7034581899642944, 0.40625]\n",
      "epoch 22 batch 15\n",
      "loss & accuracy [0.6191283464431763, 0.59375]\n",
      "epoch 23 batch 0\n",
      "loss & accuracy [0.6215664148330688, 0.5625]\n",
      "epoch 23 batch 5\n",
      "loss & accuracy [0.5552745461463928, 0.5625]\n",
      "epoch 23 batch 10\n",
      "loss & accuracy [0.6082116961479187, 0.3125]\n",
      "epoch 23 batch 15\n",
      "loss & accuracy [0.6207265853881836, 0.4375]\n",
      "epoch 24 batch 0\n",
      "loss & accuracy [0.6298585534095764, 0.4375]\n",
      "epoch 24 batch 5\n",
      "loss & accuracy [0.6721439957618713, 0.34375]\n",
      "epoch 24 batch 10\n",
      "loss & accuracy [0.6388969421386719, 0.375]\n",
      "epoch 24 batch 15\n",
      "loss & accuracy [0.5915995240211487, 0.5625]\n",
      "epoch 25 batch 0\n",
      "loss & accuracy [0.6140210628509521, 0.4375]\n",
      "epoch 25 batch 5\n",
      "loss & accuracy [0.6641519069671631, 0.375]\n",
      "epoch 25 batch 10\n",
      "loss & accuracy [0.555261492729187, 0.40625]\n",
      "epoch 25 batch 15\n",
      "loss & accuracy [0.5623918771743774, 0.5]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "epoch 25\n",
      "f1 score 0.5454545454545454\n",
      "average f1 score 0.5810452349582784\n",
      "epoch 26 batch 0\n",
      "loss & accuracy [0.652411937713623, 0.53125]\n",
      "epoch 26 batch 5\n",
      "loss & accuracy [0.5909062623977661, 0.4375]\n",
      "epoch 26 batch 10\n",
      "loss & accuracy [0.6359883546829224, 0.53125]\n",
      "epoch 26 batch 15\n",
      "loss & accuracy [0.6169657707214355, 0.34375]\n",
      "epoch 27 batch 0\n",
      "loss & accuracy [0.5873735547065735, 0.34375]\n",
      "epoch 27 batch 5\n",
      "loss & accuracy [0.6408942937850952, 0.34375]\n",
      "epoch 27 batch 10\n",
      "loss & accuracy [0.6141749024391174, 0.46875]\n",
      "epoch 27 batch 15\n",
      "loss & accuracy [0.609618067741394, 0.4375]\n",
      "epoch 28 batch 0\n",
      "loss & accuracy [0.6409671306610107, 0.46875]\n",
      "epoch 28 batch 5\n",
      "loss & accuracy [0.6530758142471313, 0.5]\n",
      "epoch 28 batch 10\n",
      "loss & accuracy [0.6522326469421387, 0.53125]\n",
      "epoch 28 batch 15\n",
      "loss & accuracy [0.5827107429504395, 0.5]\n",
      "epoch 29 batch 0\n",
      "loss & accuracy [0.709791898727417, 0.5]\n",
      "epoch 29 batch 5\n",
      "loss & accuracy [0.6794811487197876, 0.5]\n",
      "epoch 29 batch 10\n",
      "loss & accuracy [0.687023937702179, 0.59375]\n",
      "epoch 29 batch 15\n",
      "loss & accuracy [0.6894758939743042, 0.625]\n",
      "epoch 30 batch 0\n",
      "loss & accuracy [0.7002842426300049, 0.5625]\n",
      "epoch 30 batch 5\n",
      "loss & accuracy [0.629425048828125, 0.59375]\n",
      "epoch 30 batch 10\n",
      "loss & accuracy [0.6254807114601135, 0.5625]\n",
      "epoch 30 batch 15\n",
      "loss & accuracy [0.6298537254333496, 0.53125]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "epoch 30\n",
      "f1 score 0.72\n",
      "average f1 score 0.6008959156785244\n",
      "epoch 31 batch 0\n",
      "loss & accuracy [0.6474555730819702, 0.6875]\n",
      "epoch 31 batch 5\n",
      "loss & accuracy [0.7792617082595825, 0.375]\n",
      "epoch 31 batch 10\n",
      "loss & accuracy [0.5981131792068481, 0.34375]\n",
      "epoch 31 batch 15\n",
      "loss & accuracy [0.5932739973068237, 0.53125]\n",
      "epoch 32 batch 0\n",
      "loss & accuracy [0.6442385315895081, 0.375]\n",
      "epoch 32 batch 5\n",
      "loss & accuracy [0.6481683850288391, 0.4375]\n",
      "epoch 32 batch 10\n",
      "loss & accuracy [0.5830817222595215, 0.375]\n",
      "epoch 32 batch 15\n",
      "loss & accuracy [0.5921112298965454, 0.5625]\n",
      "epoch 33 batch 0\n",
      "loss & accuracy [0.6384650468826294, 0.34375]\n",
      "epoch 33 batch 5\n",
      "loss & accuracy [0.6932767629623413, 0.40625]\n",
      "epoch 33 batch 10\n",
      "loss & accuracy [0.5575273633003235, 0.375]\n",
      "epoch 33 batch 15\n",
      "loss & accuracy [0.6337058544158936, 0.5625]\n",
      "epoch 34 batch 0\n",
      "loss & accuracy [0.6690753698348999, 0.65625]\n",
      "epoch 34 batch 5\n",
      "loss & accuracy [0.6363024115562439, 0.5]\n",
      "epoch 34 batch 10\n",
      "loss & accuracy [0.5739313364028931, 0.4375]\n",
      "epoch 34 batch 15\n",
      "loss & accuracy [0.6146946549415588, 0.46875]\n",
      "epoch 35 batch 0\n",
      "loss & accuracy [0.6822808980941772, 0.53125]\n",
      "epoch 35 batch 5\n",
      "loss & accuracy [0.5382913947105408, 0.46875]\n",
      "epoch 35 batch 10\n",
      "loss & accuracy [0.6191492080688477, 0.4375]\n",
      "epoch 35 batch 15\n",
      "loss & accuracy [0.628494918346405, 0.46875]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "epoch 35\n",
      "f1 score 0.72\n",
      "average f1 score 0.6157839262187088\n",
      "epoch 36 batch 0\n",
      "loss & accuracy [0.5985243320465088, 0.5]\n",
      "epoch 36 batch 5\n",
      "loss & accuracy [0.6726563572883606, 0.4375]\n",
      "epoch 36 batch 10\n",
      "loss & accuracy [0.5825397968292236, 0.34375]\n",
      "epoch 36 batch 15\n",
      "loss & accuracy [0.567365288734436, 0.3125]\n",
      "epoch 37 batch 0\n",
      "loss & accuracy [0.5283505916595459, 0.5]\n",
      "epoch 37 batch 5\n",
      "loss & accuracy [0.5412200689315796, 0.5]\n",
      "epoch 37 batch 10\n",
      "loss & accuracy [0.6563884019851685, 0.375]\n",
      "epoch 37 batch 15\n",
      "loss & accuracy [0.5824568867683411, 0.40625]\n",
      "epoch 38 batch 0\n",
      "loss & accuracy [0.5629405379295349, 0.5]\n",
      "epoch 38 batch 5\n",
      "loss & accuracy [0.6298320293426514, 0.59375]\n",
      "epoch 38 batch 10\n",
      "loss & accuracy [0.6167585253715515, 0.375]\n",
      "epoch 38 batch 15\n",
      "loss & accuracy [0.7043038606643677, 0.40625]\n",
      "epoch 39 batch 0\n",
      "loss & accuracy [0.6199350953102112, 0.59375]\n",
      "epoch 39 batch 5\n",
      "loss & accuracy [0.5682713985443115, 0.46875]\n",
      "epoch 39 batch 10\n",
      "loss & accuracy [0.5335513949394226, 0.4375]\n",
      "epoch 39 batch 15\n",
      "loss & accuracy [0.6413042545318604, 0.5]\n",
      "epoch 40 batch 0\n",
      "loss & accuracy [0.5241873264312744, 0.375]\n",
      "epoch 40 batch 5\n",
      "loss & accuracy [0.6581052541732788, 0.25]\n",
      "epoch 40 batch 10\n",
      "loss & accuracy [0.4793844223022461, 0.40625]\n",
      "epoch 40 batch 15\n",
      "loss & accuracy [0.5428317189216614, 0.5]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "epoch 40\n",
      "f1 score 0.5454545454545454\n",
      "average f1 score 0.6079695505782462\n",
      "epoch 41 batch 0\n",
      "loss & accuracy [0.6325511336326599, 0.46875]\n",
      "epoch 41 batch 5\n",
      "loss & accuracy [0.541161060333252, 0.4375]\n",
      "epoch 41 batch 10\n",
      "loss & accuracy [0.551044225692749, 0.4375]\n",
      "epoch 41 batch 15\n",
      "loss & accuracy [0.6689999103546143, 0.40625]\n",
      "epoch 42 batch 0\n",
      "loss & accuracy [0.5822842121124268, 0.53125]\n",
      "epoch 42 batch 5\n",
      "loss & accuracy [0.6173547506332397, 0.5]\n",
      "epoch 42 batch 10\n",
      "loss & accuracy [0.6072851419448853, 0.5]\n",
      "epoch 42 batch 15\n",
      "loss & accuracy [0.669100284576416, 0.46875]\n",
      "epoch 43 batch 0\n",
      "loss & accuracy [0.6404634714126587, 0.4375]\n",
      "epoch 43 batch 5\n",
      "loss & accuracy [0.5525197386741638, 0.34375]\n",
      "epoch 43 batch 10\n",
      "loss & accuracy [0.5974310636520386, 0.375]\n",
      "epoch 43 batch 15\n",
      "loss & accuracy [0.5043059587478638, 0.46875]\n",
      "epoch 44 batch 0\n",
      "loss & accuracy [0.6463704109191895, 0.46875]\n",
      "epoch 44 batch 5\n",
      "loss & accuracy [0.5373440384864807, 0.4375]\n",
      "epoch 44 batch 10\n",
      "loss & accuracy [0.4825301170349121, 0.5625]\n",
      "epoch 44 batch 15\n",
      "loss & accuracy [0.608356237411499, 0.4375]\n",
      "epoch 45 batch 0\n",
      "loss & accuracy [0.5758862495422363, 0.5]\n",
      "epoch 45 batch 5\n",
      "loss & accuracy [0.6529510021209717, 0.4375]\n",
      "epoch 45 batch 10\n",
      "loss & accuracy [0.5849483013153076, 0.4375]\n",
      "epoch 45 batch 15\n",
      "loss & accuracy [0.5749691128730774, 0.5625]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "epoch 45\n",
      "f1 score 0.5454545454545454\n",
      "average f1 score 0.6017180500658761\n",
      "epoch 46 batch 0\n",
      "loss & accuracy [0.6478389501571655, 0.46875]\n",
      "epoch 46 batch 5\n",
      "loss & accuracy [0.6846432685852051, 0.46875]\n",
      "epoch 46 batch 10\n",
      "loss & accuracy [0.607117772102356, 0.40625]\n",
      "epoch 46 batch 15\n",
      "loss & accuracy [0.4569438695907593, 0.4375]\n",
      "epoch 47 batch 0\n",
      "loss & accuracy [0.5865678787231445, 0.40625]\n",
      "epoch 47 batch 5\n",
      "loss & accuracy [0.41147685050964355, 0.5]\n",
      "epoch 47 batch 10\n",
      "loss & accuracy [0.5468246936798096, 0.625]\n",
      "epoch 47 batch 15\n",
      "loss & accuracy [0.543906569480896, 0.28125]\n",
      "epoch 48 batch 0\n",
      "loss & accuracy [0.6262547969818115, 0.46875]\n",
      "epoch 48 batch 5\n",
      "loss & accuracy [0.48897305130958557, 0.46875]\n",
      "epoch 48 batch 10\n",
      "loss & accuracy [0.40435755252838135, 0.5625]\n",
      "epoch 48 batch 15\n",
      "loss & accuracy [0.47154268622398376, 0.46875]\n",
      "epoch 49 batch 0\n",
      "loss & accuracy [0.6265067458152771, 0.59375]\n",
      "epoch 49 batch 5\n",
      "loss & accuracy [0.5651443600654602, 0.5]\n",
      "epoch 49 batch 10\n",
      "loss & accuracy [0.5502200126647949, 0.34375]\n",
      "epoch 49 batch 15\n",
      "loss & accuracy [0.49832186102867126, 0.40625]\n",
      "epoch 50 batch 0\n",
      "loss & accuracy [0.490575909614563, 0.53125]\n",
      "epoch 50 batch 5\n",
      "loss & accuracy [0.45500996708869934, 0.46875]\n",
      "epoch 50 batch 10\n",
      "loss & accuracy [0.36078232526779175, 0.5625]\n",
      "epoch 50 batch 15\n",
      "loss & accuracy [0.3722710609436035, 0.5]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "epoch 50\n",
      "f1 score 0.72\n",
      "average f1 score 0.612470954605342\n",
      "epoch 51 batch 0\n",
      "loss & accuracy [0.48611193895339966, 0.28125]\n",
      "epoch 51 batch 5\n",
      "loss & accuracy [0.3056369423866272, 0.46875]\n",
      "epoch 51 batch 10\n",
      "loss & accuracy [0.23998916149139404, 0.5]\n",
      "epoch 51 batch 15\n",
      "loss & accuracy [0.6210325956344604, 0.53125]\n",
      "epoch 52 batch 0\n",
      "loss & accuracy [0.46993324160575867, 0.46875]\n",
      "epoch 52 batch 5\n",
      "loss & accuracy [0.3954557180404663, 0.5]\n",
      "epoch 52 batch 10\n",
      "loss & accuracy [0.3835442066192627, 0.46875]\n",
      "epoch 52 batch 15\n",
      "loss & accuracy [0.4658920168876648, 0.4375]\n",
      "epoch 53 batch 0\n",
      "loss & accuracy [0.43029436469078064, 0.4375]\n",
      "epoch 53 batch 5\n",
      "loss & accuracy [0.5009287595748901, 0.40625]\n",
      "epoch 53 batch 10\n",
      "loss & accuracy [0.49417218565940857, 0.375]\n",
      "epoch 53 batch 15\n",
      "loss & accuracy [0.4848206639289856, 0.3125]\n",
      "epoch 54 batch 0\n",
      "loss & accuracy [0.5174126625061035, 0.625]\n",
      "epoch 54 batch 5\n",
      "loss & accuracy [0.5868879556655884, 0.375]\n",
      "epoch 54 batch 10\n",
      "loss & accuracy [0.6158444881439209, 0.40625]\n",
      "epoch 54 batch 15\n",
      "loss & accuracy [0.36351266503334045, 0.34375]\n",
      "epoch 55 batch 0\n",
      "loss & accuracy [0.29831185936927795, 0.40625]\n",
      "epoch 55 batch 5\n",
      "loss & accuracy [0.3740466833114624, 0.46875]\n",
      "epoch 55 batch 10\n",
      "loss & accuracy [0.2643561363220215, 0.375]\n",
      "epoch 55 batch 15\n",
      "loss & accuracy [0.41482600569725037, 0.375]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "epoch 55\n",
      "f1 score 0.5777777777777777\n",
      "average f1 score 0.6095798565363783\n",
      "epoch 56 batch 0\n",
      "loss & accuracy [0.4485735297203064, 0.4375]\n",
      "epoch 56 batch 5\n",
      "loss & accuracy [0.399588942527771, 0.53125]\n",
      "epoch 56 batch 10\n",
      "loss & accuracy [0.3599889576435089, 0.53125]\n",
      "epoch 56 batch 15\n",
      "loss & accuracy [0.4958610534667969, 0.40625]\n",
      "epoch 57 batch 0\n",
      "loss & accuracy [0.28197717666625977, 0.40625]\n",
      "epoch 57 batch 5\n",
      "loss & accuracy [0.3129241466522217, 0.4375]\n",
      "epoch 57 batch 10\n",
      "loss & accuracy [0.3950238525867462, 0.625]\n",
      "epoch 57 batch 15\n",
      "loss & accuracy [0.44014889001846313, 0.40625]\n",
      "epoch 58 batch 0\n",
      "loss & accuracy [0.4079836308956146, 0.5625]\n",
      "epoch 58 batch 5\n",
      "loss & accuracy [0.4906231164932251, 0.5]\n",
      "epoch 58 batch 10\n",
      "loss & accuracy [0.29658469557762146, 0.46875]\n",
      "epoch 58 batch 15\n",
      "loss & accuracy [0.34162452816963196, 0.46875]\n",
      "epoch 59 batch 0\n",
      "loss & accuracy [0.19994528591632843, 0.5625]\n",
      "epoch 59 batch 5\n",
      "loss & accuracy [0.3993237614631653, 0.4375]\n",
      "epoch 59 batch 10\n",
      "loss & accuracy [0.3352207541465759, 0.46875]\n",
      "epoch 59 batch 15\n",
      "loss & accuracy [0.24880334734916687, 0.34375]\n",
      "epoch 60 batch 0\n",
      "loss & accuracy [0.3827117681503296, 0.5625]\n",
      "epoch 60 batch 5\n",
      "loss & accuracy [0.48622506856918335, 0.375]\n",
      "epoch 60 batch 10\n",
      "loss & accuracy [0.35330015420913696, 0.40625]\n",
      "epoch 60 batch 15\n",
      "loss & accuracy [0.29778745770454407, 0.59375]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "epoch 60\n",
      "f1 score 0.5454545454545454\n",
      "average f1 score 0.6046471402993142\n",
      "epoch 61 batch 0\n",
      "loss & accuracy [0.4328685402870178, 0.3125]\n",
      "epoch 61 batch 5\n",
      "loss & accuracy [0.4473355710506439, 0.53125]\n",
      "epoch 61 batch 10\n",
      "loss & accuracy [0.3454773426055908, 0.5]\n",
      "epoch 61 batch 15\n",
      "loss & accuracy [0.4171105921268463, 0.40625]\n",
      "epoch 62 batch 0\n",
      "loss & accuracy [0.2596527338027954, 0.5]\n",
      "epoch 62 batch 5\n",
      "loss & accuracy [0.28533491492271423, 0.375]\n",
      "epoch 62 batch 10\n",
      "loss & accuracy [0.2005597949028015, 0.5]\n",
      "epoch 62 batch 15\n",
      "loss & accuracy [0.25526687502861023, 0.34375]\n",
      "epoch 63 batch 0\n",
      "loss & accuracy [0.27433669567108154, 0.5625]\n",
      "epoch 63 batch 5\n",
      "loss & accuracy [0.2856125831604004, 0.4375]\n",
      "epoch 63 batch 10\n",
      "loss & accuracy [0.24274438619613647, 0.59375]\n",
      "epoch 63 batch 15\n",
      "loss & accuracy [0.2858564257621765, 0.4375]\n",
      "epoch 64 batch 0\n",
      "loss & accuracy [0.34885549545288086, 0.5625]\n",
      "epoch 64 batch 5\n",
      "loss & accuracy [0.4965652823448181, 0.59375]\n",
      "epoch 64 batch 10\n",
      "loss & accuracy [0.2790583372116089, 0.625]\n",
      "epoch 64 batch 15\n",
      "loss & accuracy [0.2368617206811905, 0.4375]\n",
      "epoch 65 batch 0\n",
      "loss & accuracy [0.2594919800758362, 0.5625]\n",
      "epoch 65 batch 5\n",
      "loss & accuracy [0.29273781180381775, 0.53125]\n",
      "epoch 65 batch 10\n",
      "loss & accuracy [0.17335233092308044, 0.375]\n",
      "epoch 65 batch 15\n",
      "loss & accuracy [0.25246426463127136, 0.375]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "epoch 65\n",
      "f1 score 0.6086956521739131\n",
      "average f1 score 0.6049363197189284\n",
      "epoch 66 batch 0\n",
      "loss & accuracy [0.3151475787162781, 0.375]\n",
      "epoch 66 batch 5\n",
      "loss & accuracy [0.26769161224365234, 0.46875]\n",
      "epoch 66 batch 10\n",
      "loss & accuracy [0.09449979662895203, 0.4375]\n",
      "epoch 66 batch 15\n",
      "loss & accuracy [0.14208301901817322, 0.4375]\n",
      "epoch 67 batch 0\n",
      "loss & accuracy [0.24328798055648804, 0.59375]\n",
      "epoch 67 batch 5\n",
      "loss & accuracy [0.11873501539230347, 0.5]\n",
      "epoch 67 batch 10\n",
      "loss & accuracy [0.1153937503695488, 0.375]\n",
      "epoch 67 batch 15\n",
      "loss & accuracy [0.39833909273147583, 0.46875]\n",
      "epoch 68 batch 0\n",
      "loss & accuracy [0.19995558261871338, 0.46875]\n",
      "epoch 68 batch 5\n",
      "loss & accuracy [0.34407585859298706, 0.375]\n",
      "epoch 68 batch 10\n",
      "loss & accuracy [0.3320111036300659, 0.4375]\n",
      "epoch 68 batch 15\n",
      "loss & accuracy [0.1704665720462799, 0.5]\n",
      "epoch 69 batch 0\n",
      "loss & accuracy [0.17058226466178894, 0.4375]\n",
      "epoch 69 batch 5\n",
      "loss & accuracy [0.2065775990486145, 0.4375]\n",
      "epoch 69 batch 10\n",
      "loss & accuracy [0.15846918523311615, 0.40625]\n",
      "epoch 69 batch 15\n",
      "loss & accuracy [0.1949741244316101, 0.40625]\n",
      "epoch 70 batch 0\n",
      "loss & accuracy [0.12077579647302628, 0.46875]\n",
      "epoch 70 batch 5\n",
      "loss & accuracy [0.12859255075454712, 0.5]\n",
      "epoch 70 batch 10\n",
      "loss & accuracy [0.11545916646718979, 0.40625]\n",
      "epoch 70 batch 15\n",
      "loss & accuracy [0.14560791850090027, 0.46875]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "epoch 70\n",
      "f1 score 0.6938775510204082\n",
      "average f1 score 0.6108657351390271\n",
      "epoch 71 batch 0\n",
      "loss & accuracy [0.24483969807624817, 0.5625]\n",
      "epoch 71 batch 5\n",
      "loss & accuracy [0.20952880382537842, 0.375]\n",
      "epoch 71 batch 10\n",
      "loss & accuracy [0.14371755719184875, 0.4375]\n",
      "epoch 71 batch 15\n",
      "loss & accuracy [0.36897730827331543, 0.4375]\n",
      "epoch 72 batch 0\n",
      "loss & accuracy [0.106252521276474, 0.40625]\n",
      "epoch 72 batch 5\n",
      "loss & accuracy [0.1362161636352539, 0.5625]\n",
      "epoch 72 batch 10\n",
      "loss & accuracy [0.10088460147380829, 0.4375]\n",
      "epoch 72 batch 15\n",
      "loss & accuracy [0.09886687248945236, 0.5625]\n",
      "epoch 73 batch 0\n",
      "loss & accuracy [0.06348538398742676, 0.40625]\n",
      "epoch 73 batch 5\n",
      "loss & accuracy [0.050980664789676666, 0.4375]\n",
      "epoch 73 batch 10\n",
      "loss & accuracy [0.08309071511030197, 0.5625]\n",
      "epoch 73 batch 15\n",
      "loss & accuracy [0.07079315185546875, 0.46875]\n",
      "epoch 74 batch 0\n",
      "loss & accuracy [0.08177414536476135, 0.625]\n",
      "epoch 74 batch 5\n",
      "loss & accuracy [0.045132484287023544, 0.5]\n",
      "epoch 74 batch 10\n",
      "loss & accuracy [0.0650203749537468, 0.34375]\n",
      "epoch 74 batch 15\n",
      "loss & accuracy [0.1691376268863678, 0.59375]\n",
      "epoch 75 batch 0\n",
      "loss & accuracy [0.2032863348722458, 0.53125]\n",
      "epoch 75 batch 5\n",
      "loss & accuracy [0.08003990352153778, 0.46875]\n",
      "epoch 75 batch 10\n",
      "loss & accuracy [0.1614382266998291, 0.53125]\n",
      "epoch 75 batch 15\n",
      "loss & accuracy [0.026447158306837082, 0.4375]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "epoch 75\n",
      "f1 score 0.6666666666666666\n",
      "average f1 score 0.6143532933595045\n",
      "epoch 76 batch 0\n",
      "loss & accuracy [0.06551557779312134, 0.5625]\n",
      "epoch 76 batch 5\n",
      "loss & accuracy [0.16119493544101715, 0.4375]\n",
      "epoch 76 batch 10\n",
      "loss & accuracy [0.0318453349173069, 0.5]\n",
      "epoch 76 batch 15\n",
      "loss & accuracy [0.13836023211479187, 0.3125]\n",
      "epoch 77 batch 0\n",
      "loss & accuracy [0.1277371048927307, 0.5]\n",
      "epoch 77 batch 5\n",
      "loss & accuracy [0.1159340888261795, 0.46875]\n",
      "epoch 77 batch 10\n",
      "loss & accuracy [0.10946857929229736, 0.46875]\n",
      "epoch 77 batch 15\n",
      "loss & accuracy [0.08405628055334091, 0.46875]\n",
      "epoch 78 batch 0\n",
      "loss & accuracy [0.46347829699516296, 0.59375]\n",
      "epoch 78 batch 5\n",
      "loss & accuracy [0.28097379207611084, 0.625]\n",
      "epoch 78 batch 10\n",
      "loss & accuracy [0.24741637706756592, 0.4375]\n",
      "epoch 78 batch 15\n",
      "loss & accuracy [0.23785868287086487, 0.4375]\n",
      "epoch 79 batch 0\n",
      "loss & accuracy [0.3483937084674835, 0.375]\n",
      "epoch 79 batch 5\n",
      "loss & accuracy [0.3026599884033203, 0.5625]\n",
      "epoch 79 batch 10\n",
      "loss & accuracy [0.06030915677547455, 0.34375]\n",
      "epoch 79 batch 15\n",
      "loss & accuracy [0.08241342008113861, 0.53125]\n",
      "epoch 80 batch 0\n",
      "loss & accuracy [0.11679395288228989, 0.59375]\n",
      "epoch 80 batch 5\n",
      "loss & accuracy [0.09058517962694168, 0.5]\n",
      "epoch 80 batch 10\n",
      "loss & accuracy [0.2002905011177063, 0.3125]\n",
      "epoch 80 batch 15\n",
      "loss & accuracy [0.0990304946899414, 0.53125]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "epoch 80\n",
      "f1 score 0.5454545454545454\n",
      "average f1 score 0.6103004258356833\n",
      "epoch 81 batch 0\n",
      "loss & accuracy [0.08546328544616699, 0.625]\n",
      "epoch 81 batch 5\n",
      "loss & accuracy [0.03701353818178177, 0.25]\n",
      "epoch 81 batch 10\n",
      "loss & accuracy [0.07703845202922821, 0.375]\n",
      "epoch 81 batch 15\n",
      "loss & accuracy [0.23970773816108704, 0.4375]\n",
      "epoch 82 batch 0\n",
      "loss & accuracy [0.07958146929740906, 0.40625]\n",
      "epoch 82 batch 5\n",
      "loss & accuracy [0.1630273163318634, 0.59375]\n",
      "epoch 82 batch 10\n",
      "loss & accuracy [0.038427241146564484, 0.5625]\n",
      "epoch 82 batch 15\n",
      "loss & accuracy [0.03946469724178314, 0.40625]\n",
      "epoch 83 batch 0\n",
      "loss & accuracy [0.04489516466856003, 0.5625]\n",
      "epoch 83 batch 5\n",
      "loss & accuracy [0.11068008095026016, 0.28125]\n",
      "epoch 83 batch 10\n",
      "loss & accuracy [0.03863126412034035, 0.40625]\n",
      "epoch 83 batch 15\n",
      "loss & accuracy [0.0124056376516819, 0.40625]\n",
      "epoch 84 batch 0\n",
      "loss & accuracy [0.07316634058952332, 0.4375]\n",
      "epoch 84 batch 5\n",
      "loss & accuracy [0.07851295173168182, 0.625]\n",
      "epoch 84 batch 10\n",
      "loss & accuracy [0.05219874531030655, 0.5]\n",
      "epoch 84 batch 15\n",
      "loss & accuracy [0.03153138980269432, 0.53125]\n",
      "epoch 85 batch 0\n",
      "loss & accuracy [0.0664571151137352, 0.53125]\n",
      "epoch 85 batch 5\n",
      "loss & accuracy [0.06792017817497253, 0.5]\n",
      "epoch 85 batch 10\n",
      "loss & accuracy [0.17512042820453644, 0.53125]\n",
      "epoch 85 batch 15\n",
      "loss & accuracy [0.06405311822891235, 0.53125]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "epoch 85\n",
      "f1 score 0.6938775510204082\n",
      "average f1 score 0.614943599457057\n",
      "epoch 86 batch 0\n",
      "loss & accuracy [0.07812388986349106, 0.40625]\n",
      "epoch 86 batch 5\n",
      "loss & accuracy [0.07218869775533676, 0.5]\n",
      "epoch 86 batch 10\n",
      "loss & accuracy [0.0331135019659996, 0.5625]\n",
      "epoch 86 batch 15\n",
      "loss & accuracy [0.15234875679016113, 0.65625]\n",
      "epoch 87 batch 0\n",
      "loss & accuracy [0.18706607818603516, 0.4375]\n",
      "epoch 87 batch 5\n",
      "loss & accuracy [0.15144634246826172, 0.4375]\n",
      "epoch 87 batch 10\n",
      "loss & accuracy [0.15085715055465698, 0.53125]\n",
      "epoch 87 batch 15\n",
      "loss & accuracy [0.020547769963741302, 0.4375]\n",
      "epoch 88 batch 0\n",
      "loss & accuracy [0.07459084689617157, 0.5]\n",
      "epoch 88 batch 5\n",
      "loss & accuracy [0.039651237428188324, 0.53125]\n",
      "epoch 88 batch 10\n",
      "loss & accuracy [0.04410585016012192, 0.5]\n",
      "epoch 88 batch 15\n",
      "loss & accuracy [0.028998425230383873, 0.46875]\n",
      "epoch 89 batch 0\n",
      "loss & accuracy [0.01773039437830448, 0.625]\n",
      "epoch 89 batch 5\n",
      "loss & accuracy [0.04900794476270676, 0.375]\n",
      "epoch 89 batch 10\n",
      "loss & accuracy [0.0484880693256855, 0.34375]\n",
      "epoch 89 batch 15\n",
      "loss & accuracy [0.015900854021310806, 0.375]\n",
      "epoch 90 batch 0\n",
      "loss & accuracy [0.0224014762789011, 0.5625]\n",
      "epoch 90 batch 5\n",
      "loss & accuracy [0.015278063714504242, 0.40625]\n",
      "epoch 90 batch 10\n",
      "loss & accuracy [0.012394934892654419, 0.375]\n",
      "epoch 90 batch 15\n",
      "loss & accuracy [0.013757644221186638, 0.53125]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "epoch 90\n",
      "f1 score 0.6666666666666666\n",
      "average f1 score 0.6176658661522996\n",
      "epoch 91 batch 0\n",
      "loss & accuracy [0.035030245780944824, 0.59375]\n",
      "epoch 91 batch 5\n",
      "loss & accuracy [0.01011644583195448, 0.75]\n",
      "epoch 91 batch 10\n",
      "loss & accuracy [0.010092219337821007, 0.21875]\n",
      "epoch 91 batch 15\n",
      "loss & accuracy [0.01508030854165554, 0.5]\n",
      "epoch 92 batch 0\n",
      "loss & accuracy [0.0045725018717348576, 0.4375]\n",
      "epoch 92 batch 5\n",
      "loss & accuracy [0.012272115796804428, 0.5]\n",
      "epoch 92 batch 10\n",
      "loss & accuracy [0.014131546020507812, 0.5]\n",
      "epoch 92 batch 15\n",
      "loss & accuracy [0.03349000960588455, 0.34375]\n",
      "epoch 93 batch 0\n",
      "loss & accuracy [0.005100581329315901, 0.46875]\n",
      "epoch 93 batch 5\n",
      "loss & accuracy [0.007112244144082069, 0.6875]\n",
      "epoch 93 batch 10\n",
      "loss & accuracy [0.006386685650795698, 0.5625]\n",
      "epoch 93 batch 15\n",
      "loss & accuracy [0.003872295143082738, 0.5]\n",
      "epoch 94 batch 0\n",
      "loss & accuracy [0.003936839289963245, 0.65625]\n",
      "epoch 94 batch 5\n",
      "loss & accuracy [0.09289044141769409, 0.5]\n",
      "epoch 94 batch 10\n",
      "loss & accuracy [0.005567212123423815, 0.46875]\n",
      "epoch 94 batch 15\n",
      "loss & accuracy [0.032685812562704086, 0.3125]\n",
      "epoch 95 batch 0\n",
      "loss & accuracy [0.012918859720230103, 0.4375]\n",
      "epoch 95 batch 5\n",
      "loss & accuracy [0.0014313255669549108, 0.46875]\n",
      "epoch 95 batch 10\n",
      "loss & accuracy [0.012658428400754929, 0.5]\n",
      "epoch 95 batch 15\n",
      "loss & accuracy [0.002959236968308687, 0.40625]\n",
      "+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
      "epoch 95\n",
      "f1 score 0.6086956521739131\n",
      "average f1 score 0.6172173554533803\n",
      "epoch 96 batch 0\n",
      "loss & accuracy [0.06375949084758759, 0.34375]\n",
      "epoch 96 batch 5\n",
      "loss & accuracy [0.022154640406370163, 0.4375]\n",
      "epoch 96 batch 10\n",
      "loss & accuracy [0.01507645659148693, 0.53125]\n",
      "epoch 96 batch 15\n",
      "loss & accuracy [0.007011641748249531, 0.375]\n",
      "epoch 97 batch 0\n",
      "loss & accuracy [0.004740994423627853, 0.4375]\n",
      "epoch 97 batch 5\n",
      "loss & accuracy [0.006633864250034094, 0.5625]\n",
      "epoch 97 batch 10\n",
      "loss & accuracy [0.022300245240330696, 0.53125]\n",
      "epoch 97 batch 15\n",
      "loss & accuracy [0.008724401704967022, 0.4375]\n",
      "epoch 98 batch 0\n",
      "loss & accuracy [0.008417384698987007, 0.46875]\n",
      "epoch 98 batch 5\n",
      "loss & accuracy [0.023681439459323883, 0.59375]\n",
      "epoch 98 batch 10\n",
      "loss & accuracy [0.005873397458344698, 0.5]\n",
      "epoch 98 batch 15\n",
      "loss & accuracy [0.007703401148319244, 0.46875]\n",
      "epoch 99 batch 0\n",
      "loss & accuracy [0.005628413520753384, 0.6875]\n",
      "epoch 99 batch 5\n",
      "loss & accuracy [0.04027000814676285, 0.46875]\n",
      "epoch 99 batch 10\n",
      "loss & accuracy [0.011290211230516434, 0.40625]\n",
      "epoch 99 batch 15\n",
      "loss & accuracy [0.018740419298410416, 0.59375]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-123ccb1837be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m    \u001b[0mlabels_translation_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m    discriminator)\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"keras_model.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can't pickle weakref objects"
     ]
    }
   ],
   "source": [
    "train(32, \n",
    "   100, \n",
    "   english_translation_embeddings_train, \n",
    "   labels_translation_train, \n",
    "   english_translation_embeddings_test, \n",
    "   labels_translation_test, \n",
    "   discriminator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "pXFN5PLGFfSV"
   },
   "outputs": [],
   "source": [
    "discriminator.save('keras_model.h5')\n",
    "discriminator = keras.models.load_model('keras_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8aocLXHxhl2"
   },
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6YBZeVeRix6F",
    "outputId": "00641b59-181c-487f-87b5-fc29cf2c8ea8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The f1 score on the testing set is:\n",
      "0.640625\n"
     ]
    }
   ],
   "source": [
    "# predict on testing set and calculate the f1 score\n",
    "predictions_test = discriminator.predict_on_batch(english_translation_embeddings_test)\n",
    "print('The f1 score on the testing set is:')\n",
    "print(sklearn.metrics.f1_score(labels_translation_test, predictions_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Interview.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
