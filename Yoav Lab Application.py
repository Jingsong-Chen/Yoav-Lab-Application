# -*- coding: utf-8 -*-
"""Interview.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lEqgIs3RjT0wOMTJoxbtwqw0Vzd2Ux3h

# Link to Google Drive
"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive')
# %cd '/content/drive/My Drive/Colab Notebooks'

"""# Data pre-processing"""

import re
import numpy as np

"""### Translation blocks are sparated by empty lines:

Block 1\n

\n

Block 2\n

\n

...

Block N\n
\n
"""

# read the text file - 
train_file_path = "train.txt"
test_file_path = "test.txt"
blocks_train = open(train_file_path, "r").read().split("\n\n")
blocks_test = open(test_file_path, "r").read().split("\n\n")

# remove the trailing empty line at the end of each file
blocks_train[-1] = blocks_train[-1][:-1]
blocks_test[-1] = blocks_test[-1][:-1]

# check the last translation pair to make sure that there is no misreading
print(blocks_train[-1])
print('There are', len(blocks_train), 'training translation pairs.\n')
print(blocks_test[-1])
print('There are', len(blocks_test), 'testing translation pairs.')

"""### Each translation block has 5 lines:
*   line 1: source (Chinese)
*   line 2: translation1 (English)
*   line 3: translation2 (English)
*   line 4: probablility? (float)
*   line 5: label (human/machine)

with one exception: the second English translation of the 83th block:

`although the arms embargo to the chinese 丨 shadow on the eu summit , but still in the two sides signed several trade and other agreements .`

has one Chinese character '丨' (U+4E28). I had to manually exclude it.

Also, I encode label 'H' as 0 and 'M' as 1.


"""

# detects if texts have Chinese characters
def isChinese(texts):
  # exclude the '丨' character
  if re.search("[\u4e00-\u4e27\u4e29-\u9FFF]", texts):
      return True
  return False

# parse translation block strings
def parseBlocks(blocks):
  chinese = []
  reference = []
  english_translation = []
  probablilities = []
  labels_translation = []

  i = 0
  while i < len(blocks):
    lines = blocks[i].split('\n')
    # the first setence is Chinese
    chinese.append(lines[0])
    # the first Englisth translation is done human
    reference.append(lines[1])
    # the second English translation can be machine translation
    english_translation.append(lines[2])
    lines[4] = 0 if lines[4] == 'H' else 1
    labels_translation.append(lines[4])
    # append the quality score
    probablilities.append(float(lines[3])) 
    i += 1
  return chinese, english_translation, labels_translation, reference, probablilities

chinese_train,\
english_translation_train,\
labels_translation_train,\
reference_train,\
probablilities_train = parseBlocks(blocks_train)
labels_translation_train = np.array(labels_translation_train)
labels_reference_train = np.array([0] * len(reference_train))

# sanity check
print('******** Length check for the training info:')
print(len(chinese_train))
print(len(english_translation_train))
print(len(labels_translation_train))
print(len(reference_train))
print(len(probablilities_train), '\n')


print('******** Content check for the training info:')
print(blocks_train[99])
print(chinese_train[99])
print(english_translation_train[99])
print(labels_translation_train[99])
print(reference_train[99])
print(probablilities_train[99])

chinese_test,\
english_translation_test,\
labels_translation_test,\
reference_test,\
probablilities_test = parseBlocks(blocks_test)
labels_translation_test = np.array(labels_translation_test)
labels_reference_test = np.array([0] * len(reference_test))

# sanity check
print('******** Length check for the training info:')
print(len(chinese_test))
print(len(english_translation_test))
print(len(labels_translation_test))
print(len(reference_test))
print(len(probablilities_test), '\n')


print('******** Content check for the training info:')
print(blocks_test[99])
print(chinese_test[99])
print(english_translation_test[99])
print(labels_translation_test[99])
print(reference_test[99])
print(probablilities_test[99])

"""## Word embedding

### I use SentenceBERT to transform sentences into vectors
"""

!pip install sentence-transformers

from sentence_transformers import SentenceTransformer
import pickle

# choose model
sbert_model = SentenceTransformer('bert-base-nli-mean-tokens')

# # vectorize sentences
# english_translation_embeddings = sbert_model.encode(english_translation_train + english_translation_test)
# english_translation_embeddings_train = english_translation_embeddings[:len(english_translation_train)] # 
# english_translation_embeddings_test = english_translation_embeddings[len(english_translation_train):] # 

# # save the embeddings
path_translation_embeddings_train = 'translation_embeddings_train_bert.p'
path_translation_embeddings_test = 'translation_embeddings_test_bert.p'
# pickle.dump(english_translation_embeddings_train, open(path_translation_embeddings_train, "wb"))
# pickle.dump(english_translation_embeddings_test, open(path_translation_embeddings_test, "wb"))

# load the pickled embeddings
english_translation_embeddings_train = pickle.load(open(path_translation_embeddings_train, "rb"))
english_translation_embeddings_test = pickle.load(open(path_translation_embeddings_test, "rb"))

# sanity check
print(english_translation_embeddings_train.shape)
print(english_translation_embeddings_test.shape)

"""## Discriminator"""

import random
import keras
import sklearn
from keras.layers import Input, Dense, Flatten, Dropout, Reshape, LSTM, Bidirectional
from keras.layers import Embedding, Concatenate
from keras.layers.convolutional import Conv2D
from keras.layers.normalization import BatchNormalization
from keras.layers.advanced_activations import LeakyReLU,ReLU
from keras.optimizers import RMSprop, Adam
import keras.backend as K
from keras.models import Model, Sequential

"""Use a neuro-network."""

sentence_input = Input(shape=(768,1), name='sentence_input')

droprate = 0.3
dimension = 128

x = Dense(dimension)(sentence_input)
x = Dropout(droprate)(x)
# x = LeakyReLU(alpha=0.01)(x)
x = ReLU()(x)

x = Bidirectional(LSTM(dimension, return_sequences=True, activation='tanh'))(x) 
x = Bidirectional(LSTM(dimension, return_sequences=False, activation='tanh'))(x)

x = Dropout(droprate)(x)
x = Dense(dimension)(x)
x = ReLU()(x)

output = Dense(1, activation='softmax')(x)
discriminator = Model(sentence_input ,output)
discriminator.summary()

optimizer = Adam(lr=0.001)
discriminator.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

"""Train a model."""

def train(batch_size, iterations, training_data, training_label, testing_data, testing_label, model):
  batch_number = len(training_data) // batch_size
  avg_f1_score = 0
  for epoch in range(iterations):
    for i in range(batch_number):
      indices = np.random.choice(training_data.shape[0], batch_size, replace=False)
      batch_data = training_data[indices]
      batch_label = training_label[indices]
      # print(data.shape)
      # print(label_batch.shape)
      loss = model.train_on_batch(batch_data,batch_label)
      if i % 5 == 0:
        print('epoch', epoch, 'batch', i)
        print('loss & accuracy', loss)
    # print out progress every 5 epochs
    if epoch % 5 == 0:
      print('+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++')
      print('epoch', epoch)
      indices = np.random.choice(training_data.shape[0], batch_size, replace=False)
      batch_data = training_data[indices]
      batch_label = training_label[indices]
      predoctions = model.predict_on_batch(batch_data)
      # calculate F1 score
      f1 = sklearn.metrics.f1_score(batch_label, predoctions)
      print('f1 score', f1)
      avg_f1_score = (avg_f1_score * epoch / 5 + f1) / (epoch / 5 + 1)
      print('average f1 score', avg_f1_score)

train(32, 
   100, 
   english_translation_embeddings_train, 
   labels_translation_train, 
   english_translation_embeddings_test, 
   labels_translation_test, 
   discriminator)

discriminator.save('keras_model.h5')
discriminator = keras.models.load_model('keras_model.h5')

"""Evaluation"""

# predict on testing set and calculate the f1 score
predictions_test = discriminator.predict_on_batch(english_translation_embeddings_test)
print('The f1 score on the testing set is:')
print(sklearn.metrics.f1_score(labels_translation_test, predictions_test))